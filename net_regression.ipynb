{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "32281197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "0b469fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelCheckpoint:\n",
    "\n",
    "    def __init__(self, filepath, model):\n",
    "        self.min_loss = None\n",
    "        self.filepath = filepath\n",
    "        self.model = model\n",
    "\n",
    "    def update(self, loss):\n",
    "        if (self.min_loss is None) or (loss < self.min_loss):\n",
    "            print(\"Saving a better model\")\n",
    "            torch.save(self.model.state_dict(), self.filepath)\n",
    "            self.min_loss = loss\n",
    "            \n",
    "            \n",
    "model_path = \"best_model.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "5019f0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_dataset = pd.read_csv('BankChurners.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "0075a2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spain   6.385707944962273\n",
      "France   6.364745011086474\n",
      "Germany   6.381761287438533\n"
     ]
    }
   ],
   "source": [
    "geo_col = train_valid_dataset['Geography']\n",
    "c_col = train_valid_dataset['CreditLevel']\n",
    "\n",
    "d_count = {}\n",
    "sum_count = {}\n",
    "for i in range(0, len(geo_col)):\n",
    "    if geo_col[i] not in d_count:\n",
    "        d_count[geo_col[i]] = 1\n",
    "    else:\n",
    "        d_count[geo_col[i]] = d_count[geo_col[i]] + 1\n",
    "        \n",
    "    if geo_col[i] not in sum_count:\n",
    "        sum_count[geo_col[i]] = c_col[i]\n",
    "    else:\n",
    "        sum_count[geo_col[i]] = sum_count[geo_col[i]] + c_col[i]\n",
    "\n",
    "for item in sum_count:\n",
    "    a = sum_count[item] / d_count[item]\n",
    "    \n",
    "    print(item, \" \", a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "b093381b",
   "metadata": {},
   "outputs": [],
   "source": [
    "country = []\n",
    "for item in train_valid_dataset['Geography']:\n",
    "    if item == 'Spain':\n",
    "        country.append(1)\n",
    "    elif item == 'France':\n",
    "        country.append(2)\n",
    "    else:\n",
    "        country.append(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "3c5173a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sum = 0\n",
    "count = 0\n",
    "res = []\n",
    "for item in train_valid_dataset['Balance']:\n",
    "    if item != 0:\n",
    "        count += 1\n",
    "    sum += item\n",
    "\n",
    "for item in train_valid_dataset['Balance']:\n",
    "    if item != 0:\n",
    "        res.append(item)\n",
    "    else:\n",
    "        res.append(sum / count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e09f5c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "dfe59bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       121681.82\n",
      "1            0.00\n",
      "2       182888.08\n",
      "3       102278.79\n",
      "4       109346.13\n",
      "          ...    \n",
      "8995         0.00\n",
      "8996         0.00\n",
      "8997     98775.23\n",
      "8998    119654.44\n",
      "8999    173340.83\n",
      "Name: Balance, Length: 9000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "train_valid_dataset = train_valid_dataset.drop(columns='CustomerId')\n",
    "# train_valid_dataset = train_valid_dataset.drop(columns='Geography')\n",
    "res_col = train_valid_dataset['CreditLevel']\n",
    "train_valid_dataset = train_valid_dataset.drop(columns='CreditLevel')\n",
    "#train_valid_dataset['Balance'] = res\n",
    "train_valid_dataset['Geography'] = country\n",
    "\n",
    "\n",
    "print(train_valid_dataset['Balance'])\n",
    "\n",
    "#归一化\n",
    "train_valid_dataset = (train_valid_dataset - train_valid_dataset.min()) / (train_valid_dataset.max() - train_valid_dataset.min())\n",
    "train_valid_dataset['CreditLevel'] = res_col - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "0ddab8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ratio = 0.2 \n",
    "\n",
    "#weight_tensor = torch.tensor([len(train_valid_dataset)/(len(train_valid_dataset)-train_valid_dataset[:,-1].sum()), len(train_valid_dataset)/train_valid_dataset[:,-1].sum()]).float() \n",
    "\n",
    "nb_train = int((1.0 - valid_ratio) * len(train_valid_dataset))\n",
    "nb_valid =  int(valid_ratio * len(train_valid_dataset))\n",
    "train_dataset, valid_dataset = torch.utils.data.dataset.random_split(train_valid_dataset.to_numpy(), [nb_train, nb_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "f05a5632",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetTransformer(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, base_dataset, transform=transforms.Lambda(lambda x: x)):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        inpt, target = torch.from_numpy(self.base_dataset[index][:-1]), self.base_dataset[index][-1]\n",
    "        return self.transform(inpt).float(), int(target)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset)\n",
    "\n",
    "\n",
    "train_dataset = DatasetTransformer(train_dataset)\n",
    "valid_dataset = DatasetTransformer(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "71f511e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train set contains 7200 samples, in 72 batches\n",
      "The validation set contains 1800 samples, in 18 batches\n"
     ]
    }
   ],
   "source": [
    "#Dataloader\n",
    "\n",
    "batch_size  = 100   # Using minibatches of X samples\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "\n",
    "print(\"The train set contains {} samples, in {} batches\".format(len(train_loader.dataset), len(train_loader)))\n",
    "print(\"The validation set contains {} samples, in {} batches\".format(len(valid_loader.dataset), len(valid_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "55d0d4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define device\n",
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "83340f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FullyConnected(\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=8, out_features=14, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=14, out_features=14, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Linear(in_features=14, out_features=14, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=14, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def linear_relu(dim_in, dim_out):\n",
    "    return [nn.Linear(dim_in, dim_out),\n",
    "            nn.ReLU(inplace=True)]\n",
    "\n",
    "class FullyConnected(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(FullyConnected, self).__init__()\n",
    "        self.classifier =  nn.Sequential(\n",
    "            #nn.Dropout(0.2),\n",
    "            *linear_relu(input_size, 14),\n",
    "#             nn.Dropout(0.5), #Generally 0.2 for the input layer and 0.5 for the hidden layer\n",
    "            *linear_relu(14, 14),\n",
    "#             nn.Dropout(0.5),\n",
    "            *linear_relu(14, 14),\n",
    "#             nn.Dropout(0.5),\n",
    "            nn.Linear(14, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        y = self.classifier(x)\n",
    "        return y\n",
    "\n",
    "\n",
    "model = FullyConnected(8, 1)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "6d90788a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, f_loss, optimizer, device):\n",
    "    \"\"\"\n",
    "    Train a model for one epoch, iterating over the loader\n",
    "    using the f_loss to compute the loss and the optimizer\n",
    "    to update the parameters of the model.\n",
    "\n",
    "    Arguments :\n",
    "\n",
    "        model     -- A torch.nn.Module object\n",
    "        loader    -- A torch.utils.data.DataLoader\n",
    "        f_loss    -- The loss function, i.e. a loss Module\n",
    "        optimizer -- A torch.optim.Optimzer object\n",
    "        device    -- a torch.device class specifying the device\n",
    "                     used for computation\n",
    "\n",
    "    Returns :\n",
    "    \"\"\"\n",
    "\n",
    "    # We enter train mode. This is useless for the linear model\n",
    "    # but is important for layers such as dropout, batchnorm, ...\n",
    "    model.train()\n",
    "\n",
    "    for i, (inputs, targets) in enumerate(loader):\n",
    "        \n",
    "        \n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        # Compute the forward pass through the network up to the loss\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        loss = f_loss(outputs, targets)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "\n",
    "def test(model, loader, f_loss, device):\n",
    "    \"\"\"\n",
    "    Test a model by iterating over the loader\n",
    "\n",
    "    Arguments :\n",
    "\n",
    "        model     -- A torch.nn.Module object\n",
    "        loader    -- A torch.utils.data.DataLoader\n",
    "        f_loss    -- The loss function, i.e. a loss Module\n",
    "        device    -- The device to use for computation \n",
    "\n",
    "    Returns :\n",
    "\n",
    "        A tuple with the mean loss, mean accuracy and mean unbiaised accuracy\n",
    "\n",
    "    \"\"\"\n",
    "    # We disable gradient computation which speeds up the computation\n",
    "    # and reduces the memory usage\n",
    "    with torch.no_grad():\n",
    "        # We enter evaluation mode. This is useless for the linear model\n",
    "        # but is important with layers such as dropout, batchnorm, ..\n",
    "        model.eval()\n",
    "        N = 0\n",
    "        tot_loss, correct, unbiaised_acc = 0.0, 0.0, 0.0\n",
    "        for i, (inputs, targets) in enumerate(loader):\n",
    "\n",
    "            # We got a minibatch from the loader within inputs and targets\n",
    "\n",
    "            # We need to copy the data on the GPU if we use one\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # Compute the forward pass, i.e. the scores for each input\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # We accumulate the exact number of processed samples\n",
    "            N += inputs.shape[0]\n",
    "\n",
    "            # We accumulate the loss considering\n",
    "            # The multipliation by inputs.shape[0] is due to the fact\n",
    "            # that our loss criterion is averaging over its samples\n",
    "            tot_loss += inputs.shape[0] * f_loss(outputs, targets).item()\n",
    "\n",
    "            # For the accuracy, we compute the labels for each input\n",
    "            # Be carefull, the model is outputing scores and not the probabilities\n",
    "            # But given the softmax is not altering the rank of its input scores\n",
    "            # we can compute the label by argmaxing directly the scores\n",
    "            \n",
    "#             predicted_targets = outputs.argmax(dim=1)\n",
    "            predicted_targets = [torch.ceil(item[0]) for item in outputs]\n",
    "            \n",
    "            for i in range(0, len(targets)):\n",
    "                if targets[i] == predicted_targets[i]:\n",
    "                    correct += 1\n",
    "                \n",
    "            #print(correct)\n",
    "            #correct += (predicted_targets == targets).sum().item()\n",
    "                \n",
    "        return tot_loss/N, correct/N, unbiaised_acc/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dcc9c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "16e671ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0\n",
      " Train : Loss : 4.3289, Acc : 0.0331, Unb.Acc. : 0.0000\n",
      " Validation : Loss : 4.3114, Acc : 0.0361, Unb.Acc. : 0.0000\n",
      "Saving a better model\n",
      "\n",
      "Epoch 1\n",
      " Train : Loss : 1.5618, Acc : 0.1906, Unb.Acc. : 0.0000\n",
      " Validation : Loss : 1.5662, Acc : 0.1983, Unb.Acc. : 0.0000\n",
      "Saving a better model\n",
      "\n",
      "Epoch 2\n",
      " Train : Loss : 1.5291, Acc : 0.1900, Unb.Acc. : 0.0000\n",
      " Validation : Loss : 1.5335, Acc : 0.1978, Unb.Acc. : 0.0000\n",
      "Saving a better model\n",
      "\n",
      "Epoch 3\n",
      " Train : Loss : 1.5077, Acc : 0.1981, Unb.Acc. : 0.0000\n",
      " Validation : Loss : 1.5122, Acc : 0.1883, Unb.Acc. : 0.0000\n",
      "Saving a better model\n",
      "\n",
      "Epoch 4\n",
      " Train : Loss : 1.4946, Acc : 0.1994, Unb.Acc. : 0.0000\n",
      " Validation : Loss : 1.5001, Acc : 0.2017, Unb.Acc. : 0.0000\n",
      "Saving a better model\n",
      "\n",
      "Epoch 5\n",
      " Train : Loss : 1.4741, Acc : 0.2081, Unb.Acc. : 0.0000\n",
      " Validation : Loss : 1.4812, Acc : 0.2022, Unb.Acc. : 0.0000\n",
      "Saving a better model\n",
      "\n",
      "Epoch 6\n",
      " Train : Loss : 1.4731, Acc : 0.2047, Unb.Acc. : 0.0000\n",
      " Validation : Loss : 1.4775, Acc : 0.2056, Unb.Acc. : 0.0000\n",
      "Saving a better model\n",
      "\n",
      "Epoch 7\n",
      " Train : Loss : 1.4612, Acc : 0.2093, Unb.Acc. : 0.0000\n",
      " Validation : Loss : 1.4671, Acc : 0.2078, Unb.Acc. : 0.0000\n",
      "Saving a better model\n",
      "\n",
      "Epoch 8\n",
      " Train : Loss : 1.4585, Acc : 0.2121, Unb.Acc. : 0.0000\n",
      " Validation : Loss : 1.4655, Acc : 0.2044, Unb.Acc. : 0.0000\n",
      "Saving a better model\n",
      "\n",
      "Epoch 9\n",
      " Train : Loss : 1.4547, Acc : 0.2106, Unb.Acc. : 0.0000\n",
      " Validation : Loss : 1.4592, Acc : 0.2089, Unb.Acc. : 0.0000\n",
      "Saving a better model\n",
      "\n",
      "Epoch 10\n",
      " Train : Loss : 1.4514, Acc : 0.2117, Unb.Acc. : 0.0000\n",
      " Validation : Loss : 1.4558, Acc : 0.2083, Unb.Acc. : 0.0000\n",
      "Saving a better model\n",
      "\n",
      "Epoch 11\n",
      " Train : Loss : 1.4492, Acc : 0.2129, Unb.Acc. : 0.0000\n",
      " Validation : Loss : 1.4535, Acc : 0.2061, Unb.Acc. : 0.0000\n",
      "Saving a better model\n",
      "\n",
      "Epoch 12\n",
      " Train : Loss : 1.4478, Acc : 0.2139, Unb.Acc. : 0.0000\n",
      " Validation : Loss : 1.4512, Acc : 0.2072, Unb.Acc. : 0.0000\n",
      "Saving a better model\n",
      "\n",
      "Epoch 13\n",
      " Train : Loss : 1.4483, Acc : 0.2122, Unb.Acc. : 0.0000\n",
      " Validation : Loss : 1.4524, Acc : 0.2061, Unb.Acc. : 0.0000\n",
      "\n",
      "Epoch 14\n",
      " Train : Loss : 1.4483, Acc : 0.2124, Unb.Acc. : 0.0000\n",
      " Validation : Loss : 1.4522, Acc : 0.2056, Unb.Acc. : 0.0000\n",
      "\n",
      "Epoch 15\n",
      " Train : Loss : 1.4461, Acc : 0.2126, Unb.Acc. : 0.0000\n",
      " Validation : Loss : 1.4492, Acc : 0.2028, Unb.Acc. : 0.0000\n",
      "Saving a better model\n",
      "\n",
      "Epoch 16\n",
      " Train : Loss : 1.4514, Acc : 0.2124, Unb.Acc. : 0.0000\n",
      " Validation : Loss : 1.4565, Acc : 0.2056, Unb.Acc. : 0.0000\n",
      "\n",
      "Epoch 17\n",
      " Train : Loss : 1.4508, Acc : 0.2124, Unb.Acc. : 0.0000\n",
      " Validation : Loss : 1.4556, Acc : 0.2056, Unb.Acc. : 0.0000\n",
      "\n",
      "Epoch 18\n",
      " Train : Loss : 1.4528, Acc : 0.2124, Unb.Acc. : 0.0000\n",
      " Validation : Loss : 1.4585, Acc : 0.2056, Unb.Acc. : 0.0000\n",
      "\n",
      "Epoch 19\n",
      " Train : Loss : 1.4459, Acc : 0.2124, Unb.Acc. : 0.0000\n",
      " Validation : Loss : 1.4493, Acc : 0.2061, Unb.Acc. : 0.0000\n",
      "\n",
      "Epoch 20\n",
      " Train : Loss : 1.4484, Acc : 0.2124, Unb.Acc. : 0.0000\n",
      " Validation : Loss : 1.4526, Acc : 0.2056, Unb.Acc. : 0.0000\n",
      "\n",
      "Epoch 21\n",
      " Train : Loss : 1.4479, Acc : 0.2124, Unb.Acc. : 0.0000\n",
      " Validation : Loss : 1.4520, Acc : 0.2056, Unb.Acc. : 0.0000\n",
      "\n",
      "Epoch 22\n",
      " Train : Loss : 1.4493, Acc : 0.2124, Unb.Acc. : 0.0000\n",
      " Validation : Loss : 1.4537, Acc : 0.2056, Unb.Acc. : 0.0000\n",
      "\n",
      "Epoch 23\n",
      " Train : Loss : 1.4497, Acc : 0.2124, Unb.Acc. : 0.0000\n",
      " Validation : Loss : 1.4544, Acc : 0.2056, Unb.Acc. : 0.0000\n",
      "\n",
      "Epoch 24\n",
      " Train : Loss : 1.4451, Acc : 0.2107, Unb.Acc. : 0.0000\n",
      " Validation : Loss : 1.4482, Acc : 0.2061, Unb.Acc. : 0.0000\n",
      "Saving a better model\n",
      "\n",
      "Epoch 25\n",
      " Train : Loss : 1.4461, Acc : 0.2124, Unb.Acc. : 0.0000\n",
      " Validation : Loss : 1.4496, Acc : 0.2056, Unb.Acc. : 0.0000\n",
      "\n",
      "Epoch 26\n",
      " Train : Loss : 1.4532, Acc : 0.2124, Unb.Acc. : 0.0000\n",
      " Validation : Loss : 1.4590, Acc : 0.2056, Unb.Acc. : 0.0000\n",
      "\n",
      "Epoch 27\n",
      " Train : Loss : 1.4510, Acc : 0.2124, Unb.Acc. : 0.0000\n",
      " Validation : Loss : 1.4561, Acc : 0.2056, Unb.Acc. : 0.0000\n",
      "\n",
      "Epoch 28\n",
      " Train : Loss : 1.4517, Acc : 0.2124, Unb.Acc. : 0.0000\n",
      " Validation : Loss : 1.4571, Acc : 0.2056, Unb.Acc. : 0.0000\n",
      "\n",
      "Epoch 29\n",
      " Train : Loss : 1.4490, Acc : 0.2118, Unb.Acc. : 0.0000\n",
      " Validation : Loss : 1.4518, Acc : 0.2117, Unb.Acc. : 0.0000\n",
      "\n",
      "Epoch 30\n",
      " Train : Loss : 1.4464, Acc : 0.2104, Unb.Acc. : 0.0000\n",
      " Validation : Loss : 1.4491, Acc : 0.2061, Unb.Acc. : 0.0000\n",
      "\n",
      "Epoch 31\n",
      " Train : Loss : 1.4492, Acc : 0.2107, Unb.Acc. : 0.0000\n",
      " Validation : Loss : 1.4520, Acc : 0.2106, Unb.Acc. : 0.0000\n",
      "\n",
      "Epoch 32\n",
      " Train : Loss : 1.4486, Acc : 0.2124, Unb.Acc. : 0.0000\n",
      " Validation : Loss : 1.4529, Acc : 0.2056, Unb.Acc. : 0.0000\n",
      "\n",
      "Epoch 33\n",
      " Train : Loss : 1.4446, Acc : 0.2089, Unb.Acc. : 0.0000\n",
      " Validation : Loss : 1.4475, Acc : 0.2122, Unb.Acc. : 0.0000\n",
      "Saving a better model\n",
      "\n",
      "Epoch 34\n",
      " Train : Loss : 1.4453, Acc : 0.2124, Unb.Acc. : 0.0000\n",
      " Validation : Loss : 1.4485, Acc : 0.2056, Unb.Acc. : 0.0000\n",
      "\n",
      "Epoch 35\n",
      " Train : Loss : 1.4447, Acc : 0.2121, Unb.Acc. : 0.0000\n",
      " Validation : Loss : 1.4477, Acc : 0.2044, Unb.Acc. : 0.0000\n",
      "\n",
      "Epoch 36\n",
      " Train : Loss : 1.4506, Acc : 0.2085, Unb.Acc. : 0.0000\n",
      " Validation : Loss : 1.4533, Acc : 0.2111, Unb.Acc. : 0.0000\n",
      "\n",
      "Epoch 37\n",
      " Train : Loss : 1.4470, Acc : 0.2124, Unb.Acc. : 0.0000\n",
      " Validation : Loss : 1.4508, Acc : 0.2056, Unb.Acc. : 0.0000\n",
      "\n",
      "Epoch 38\n",
      " Train : Loss : 1.4466, Acc : 0.2124, Unb.Acc. : 0.0000\n",
      " Validation : Loss : 1.4503, Acc : 0.2056, Unb.Acc. : 0.0000\n",
      "\n",
      "Epoch 39\n",
      " Train : Loss : 1.4518, Acc : 0.2124, Unb.Acc. : 0.0000\n",
      " Validation : Loss : 1.4573, Acc : 0.2056, Unb.Acc. : 0.0000\n",
      "\n",
      "Epoch 40\n",
      " Train : Loss : 1.4455, Acc : 0.2079, Unb.Acc. : 0.0000\n",
      " Validation : Loss : 1.4484, Acc : 0.2156, Unb.Acc. : 0.0000\n",
      "\n",
      "Epoch 41\n",
      " Train : Loss : 1.4445, Acc : 0.2131, Unb.Acc. : 0.0000\n",
      " Validation : Loss : 1.4474, Acc : 0.2061, Unb.Acc. : 0.0000\n",
      "Saving a better model\n",
      "\n",
      "Epoch 42\n",
      " Train : Loss : 1.4478, Acc : 0.2124, Unb.Acc. : 0.0000\n",
      " Validation : Loss : 1.4519, Acc : 0.2056, Unb.Acc. : 0.0000\n",
      "\n",
      "Epoch 43\n",
      " Train : Loss : 1.4517, Acc : 0.2124, Unb.Acc. : 0.0000\n",
      " Validation : Loss : 1.4571, Acc : 0.2056, Unb.Acc. : 0.0000\n",
      "\n",
      "Epoch 44\n",
      " Train : Loss : 1.4471, Acc : 0.2124, Unb.Acc. : 0.0000\n",
      " Validation : Loss : 1.4508, Acc : 0.2056, Unb.Acc. : 0.0000\n",
      "\n",
      "Epoch 45\n",
      " Train : Loss : 1.4456, Acc : 0.2124, Unb.Acc. : 0.0000\n",
      " Validation : Loss : 1.4489, Acc : 0.2056, Unb.Acc. : 0.0000\n",
      "\n",
      "Epoch 46\n",
      " Train : Loss : 1.4478, Acc : 0.2124, Unb.Acc. : 0.0000\n",
      " Validation : Loss : 1.4519, Acc : 0.2056, Unb.Acc. : 0.0000\n",
      "\n",
      "Epoch 47\n",
      " Train : Loss : 1.4522, Acc : 0.2124, Unb.Acc. : 0.0000\n",
      " Validation : Loss : 1.4578, Acc : 0.2056, Unb.Acc. : 0.0000\n",
      "\n",
      "Epoch 48\n",
      " Train : Loss : 1.4468, Acc : 0.2124, Unb.Acc. : 0.0000\n",
      " Validation : Loss : 1.4507, Acc : 0.2056, Unb.Acc. : 0.0000\n",
      "\n",
      "Epoch 49\n",
      " Train : Loss : 1.4457, Acc : 0.2124, Unb.Acc. : 0.0000\n",
      " Validation : Loss : 1.4491, Acc : 0.2056, Unb.Acc. : 0.0000\n",
      "\n",
      "\n",
      " Test : Loss : 1.4474, Acc. : 0.2061, Unb.Acc. : 0.0000\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "# f_loss = torch.nn.CrossEntropyLoss()\n",
    "f_loss = torch.nn.L1Loss()\n",
    "# f_loss = torch.nn.SmoothL1Loss()\n",
    "# f_loss = torch.nn.MSELoss(reduction='mean')\n",
    "model_checkpoint = ModelCheckpoint(model_path, model)\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(\"\\nEpoch {}\".format(t))\n",
    "    train(model, train_loader, f_loss, optimizer, device)\n",
    "    train_loss, train_acc, train_unb_acc = test(model, train_loader, f_loss, device)\n",
    "    print(\" Train : Loss : {:.4f}, Acc : {:.4f}, Unb.Acc. : {:.4f}\".format(train_loss, train_acc, train_unb_acc))\n",
    "\n",
    "    val_loss, val_acc, val_unb_acc = test(model, valid_loader, f_loss, device)\n",
    "    print(\" Validation : Loss : {:.4f}, Acc : {:.4f}, Unb.Acc. : {:.4f}\".format(val_loss, val_acc, val_unb_acc))\n",
    "\n",
    "    model_checkpoint.update(val_loss)\n",
    "\n",
    "\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# Switch to eval mode \n",
    "model.eval()\n",
    "\n",
    "test_loss, test_acc, test_unb_acc = test(model, valid_loader, f_loss, device)\n",
    "print(\"\\n\\n Test : Loss : {:.4f}, Acc. : {:.4f}, Unb.Acc. : {:.4f}\".format(test_loss, test_acc, test_unb_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3676330",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c8aab9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d0f377",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
