{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7458f15c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset,Dataset,DataLoader,random_split\n",
    "\n",
    "\n",
    "def normalize(dataset):\n",
    "    dataNorm=((dataset-dataset.min())/(dataset.max()-dataset.min()))\n",
    "    dataNorm[\"CreditLevel\"]=dataset[\"CreditLevel\"]-1\n",
    "    return dataNorm\n",
    "\n",
    "# data preprocessing\n",
    "train_dataset = pd.read_csv(\"BankChurners.csv\")\n",
    "print(f\"Original data frame has {train_dataset.shape[0]} rows and {train_dataset.shape[1]} columns.\")\n",
    "train_dataset = train_dataset.drop(['CustomerId','Geography'],axis=1)\n",
    "train_dataset = normalize(train_dataset)\n",
    "\n",
    "train_dataset.info()\n",
    "print(f\"preprocessing data frame has {train_dataset.shape[0]} rows and {train_dataset.shape[1]} columns.\")\n",
    "train_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f49cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# data clean\n",
    "pd.set_option('precision',3)\n",
    "train_dataset.loc[:,['CreditLevel']].describe()\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.title('Distribution of CreditLevel')\n",
    "sns.histplot(train_dataset.CreditLevel)\n",
    "\n",
    "#heatmap\n",
    "corr = train_dataset.corr()\n",
    "plt.figure(figsize=(12,10))\n",
    "heat = sns.heatmap(data=corr)\n",
    "plt.title('Heatmap of Correlation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22773b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(train_dataset,train_dl_batch_size,test_dl_batch_size):\n",
    "    x_train = torch.tensor(train_dataset.drop('CreditLevel',axis=1).values.astype('float32'))\n",
    "    y_train = torch.tensor(train_dataset['CreditLevel'].values)\n",
    "    train_tensor = TensorDataset(x_train,y_train)\n",
    "    n_train = int(len(train_tensor)*0.8)\n",
    "    n_test = len(train_tensor) - n_train\n",
    "    train_ds,test_ds = random_split(train_tensor,[n_train,n_test])\n",
    "    train_dl = DataLoader(train_ds,batch_size=train_dl_batch_size,shuffle=True)\n",
    "    test_dl = DataLoader(test_ds,batch_size=test_dl_batch_size,shuffle=False)\n",
    "    # for features,labels in train_dl:\n",
    "    #     print(features,labels)\n",
    "    #     break\n",
    "    return train_dl,test_dl\n",
    " \n",
    "    \n",
    "train_dl_batch_size = 100\n",
    "test_dl_batch_size = 30\n",
    "train_dl,test_dl = prepare_data(train_dataset,train_dl_batch_size,test_dl_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf2076d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.nn.init import kaiming_uniform_,xavier_uniform_\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,n_inputs):\n",
    "        super(Net, self).__init__()\n",
    "        # input to first hidden layer\n",
    "        self.hidden1 = nn.Linear(n_inputs, 14)\n",
    "        kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.act1 = nn.ReLU()\n",
    "        \n",
    "        # second hidden layer\n",
    "        # self.hidden2 = nn.Linear(14, 10)\n",
    "        # kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\n",
    "        # self.dropout2 = nn.Dropout(0.3)\n",
    "        # self.act2 = nn.ReLU()\n",
    "        \n",
    "        # third hidden layer and output\n",
    "        self.hidden3 = nn.Linear(14, 10)\n",
    "        xavier_uniform_(self.hidden3.weight)\n",
    "        #self.dropout3 = nn.Dropout(0.3)\n",
    "        self.act3 = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self,X):\n",
    "         # input to first hidden layer\n",
    "        X = self.hidden1(X)\n",
    "        X = self.dropout1(X)\n",
    "        X = self.act1(X)\n",
    "        \n",
    "        # second hidden layer\n",
    "        #X = self.hidden2(X)\n",
    "        #X = self.dropout2(X)\n",
    "        #X = self.act2(X)\n",
    "        \n",
    "        # output layer\n",
    "        X = self.hidden3(X)\n",
    "        #X = self.dropout3(X)\n",
    "        X = self.act3(X)\n",
    "        return X\n",
    "\n",
    "model = Net(7)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12eb43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_dl,model):\n",
    "    #define a Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    num_epochs = 20\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs,targets) in enumerate(train_dl):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs,targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 0:   \n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 100))\n",
    "                running_loss = 0.0\n",
    "\n",
    "                \n",
    "train_model(train_dl,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6d02cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model,test_dl,test_dl_batch_size):\n",
    "    model.eval()\n",
    "    accuracy = 0.0\n",
    "    total = 0.0\n",
    "    number_of_labels = 10\n",
    "    class_correct = list(0. for i in range(number_of_labels))\n",
    "    class_total = list(0. for i in range(number_of_labels))\n",
    "    with torch.no_grad():\n",
    "        for data in test_dl:\n",
    "            inputs,targets = data\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += targets.size(0)\n",
    "            accuracy += (predicted == targets).sum().item()\n",
    "            c = (predicted == targets).squeeze()\n",
    "            for i in range(test_dl_batch_size):\n",
    "                target = targets[i]\n",
    "                class_correct[target] += c[i].item()\n",
    "                class_total[target] += 1\n",
    "    \n",
    "    accuracy = (100*accuracy/total)\n",
    "    print(f\"Accuracy for all data: {accuracy}\")\n",
    "    for i in range(number_of_labels):\n",
    "        print('Accuracy of %5d : %2d %%' % (\n",
    "            i, 100 * class_correct[i] / class_total[i]))\n",
    "        \n",
    "        \n",
    "evaluate_model(model,test_dl,test_dl_batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
