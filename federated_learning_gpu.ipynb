{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78946b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_learning_full import NeuralNet, get_bank_dataset, train\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, dataloader\n",
    "import ssl\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "\n",
    "def dirichlet_partition(training_data, testing_data, alpha, user_num):\n",
    "    idxs_train = np.arange(len(training_data))\n",
    "    idxs_valid = np.arange(len(testing_data))\n",
    "\n",
    "    labels_train = [label for _, label in training_data]\n",
    "    labels_valid = [label for _, label in testing_data]\n",
    "    # if hasattr(training_data, 'targets'):\n",
    "    #     labels_train = training_data.targets\n",
    "    #     labels_valid = testing_data.targets\n",
    "    # elif hasattr(training_data, 'img_label'):\n",
    "    #     labels_train = training_data.img_label\n",
    "    #     labels_valid = testing_data.img_label\n",
    "\n",
    "    idxs_labels_train = np.vstack((idxs_train, labels_train))\n",
    "    idxs_labels_train = idxs_labels_train[:, idxs_labels_train[1, :].argsort()]\n",
    "    idxs_labels_valid = np.vstack((idxs_valid, labels_valid))\n",
    "    idxs_labels_valid = idxs_labels_valid[:, idxs_labels_valid[1, :].argsort()]\n",
    "\n",
    "    labels = np.unique(labels_train, axis=0)\n",
    "\n",
    "    data_train_dict = data_organize(idxs_labels_train, labels)\n",
    "    data_valid_dict = data_organize(idxs_labels_valid, labels)\n",
    "\n",
    "    data_partition_profile_train = {}\n",
    "    data_partition_profile_valid = {}\n",
    "\n",
    "    for i in range(user_num):\n",
    "        data_partition_profile_train[i] = []\n",
    "        data_partition_profile_valid[i] = []\n",
    "\n",
    "    # Distribute rest data\n",
    "    for label in data_train_dict:\n",
    "        proportions = np.random.dirichlet(np.repeat(alpha, user_num))\n",
    "        proportions_train = len(data_train_dict[label]) * proportions\n",
    "        proportions_valid = len(data_valid_dict[label]) * proportions\n",
    "\n",
    "        for user in data_partition_profile_train:\n",
    "            data_partition_profile_train[user] \\\n",
    "                = set.union(set(np.random.choice(data_train_dict[label], int(proportions_train[user]), replace=False)),\n",
    "                            data_partition_profile_train[user])\n",
    "            data_train_dict[label] = list(\n",
    "                set(data_train_dict[label]) - data_partition_profile_train[user])\n",
    "\n",
    "            data_partition_profile_valid[user] = set.union(set(\n",
    "                np.random.choice(data_valid_dict[label], int(proportions_valid[user]),\n",
    "                                 replace=False)), data_partition_profile_valid[user])\n",
    "            data_valid_dict[label] = list(\n",
    "                set(data_valid_dict[label]) - data_partition_profile_valid[user])\n",
    "\n",
    "        while len(data_train_dict[label]) != 0:\n",
    "            rest_data = data_train_dict[label][0]\n",
    "            user = np.random.randint(0, user_num)\n",
    "            data_partition_profile_train[user].add(rest_data)\n",
    "            data_train_dict[label].remove(rest_data)\n",
    "\n",
    "        while len(data_valid_dict[label]) != 0:\n",
    "            rest_data = data_valid_dict[label][0]\n",
    "            user = np.random.randint(0, user_num)\n",
    "            data_partition_profile_valid[user].add(rest_data)\n",
    "            data_valid_dict[label].remove(rest_data)\n",
    "\n",
    "    for user in data_partition_profile_train:\n",
    "        data_partition_profile_train[user] = list(\n",
    "            data_partition_profile_train[user])\n",
    "        data_partition_profile_valid[user] = list(\n",
    "            data_partition_profile_valid[user])\n",
    "        np.random.shuffle(data_partition_profile_train[user])\n",
    "        np.random.shuffle(data_partition_profile_valid[user])\n",
    "\n",
    "    return data_partition_profile_train, data_partition_profile_valid\n",
    "\n",
    "\n",
    "def bank_iid(dataset, num_users):\n",
    "    num_items = int(len(dataset) / num_users)\n",
    "    dict_users, all_idxs = {}, [i for i in range(len(dataset))]\n",
    "    for i in range(num_users):\n",
    "        dict_users[i] = set(np.random.choice(all_idxs, num_items,\n",
    "                                             replace=False))\n",
    "        all_idxs = list(set(all_idxs) - dict_users[i])\n",
    "    return dict_users\n",
    "\n",
    "def bank_noniid(dataset, num_users):\n",
    "    \"\"\"\n",
    "    Sample non-I.I.D client data from MNIST dataset\n",
    "    :param dataset:\n",
    "    :param num_users:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 60,000 training imgs -->  200 imgs/shard X 300 shards\n",
    "    num_shards, num_imgs = 100, 150\n",
    "    idx_shard = [i for i in range(num_shards)]\n",
    "    dict_users = {i: np.array([]) for i in range(num_users)}\n",
    "    idxs = np.arange(num_shards*num_imgs)\n",
    "    # labels = dataset.train_labels.numpy()\n",
    "    labels = [label for _, label in dataset]\n",
    "\n",
    "    # sort labels\n",
    "    idxs_labels = np.vstack((idxs, labels))\n",
    "    idxs_labels = idxs_labels[:, idxs_labels[1, :].argsort()]\n",
    "    idxs = idxs_labels[0, :]\n",
    "\n",
    "    # divide and assign 2 shards/client\n",
    "    for i in range(num_users):\n",
    "        rand_set = set(np.random.choice(idx_shard, 2, replace=False))\n",
    "        idx_shard = list(set(idx_shard) - rand_set)\n",
    "        for rand in rand_set:\n",
    "            dict_users[i] = np.concatenate(\n",
    "                (dict_users[i], idxs[rand*num_imgs:(rand+1)*num_imgs]), axis=0)\n",
    "    return dict_users\n",
    "\n",
    "def data_organize(idxs_labels, labels):\n",
    "    data_dict = {}\n",
    "\n",
    "    labels = np.unique(labels, axis=0)\n",
    "    for one in labels:\n",
    "        data_dict[one] = []\n",
    "\n",
    "    for i in range(len(idxs_labels[1, :])):\n",
    "        data_dict[idxs_labels[1, i]].append(idxs_labels[0, i])\n",
    "    return data_dict\n",
    "\n",
    "\n",
    "class DatasetSplit(Dataset):\n",
    "    def __init__(self, dataset, idxs):\n",
    "        self.dataset = dataset\n",
    "        self.idxs = [int(i) for i in idxs]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image, label = self.dataset[self.idxs[item]]\n",
    "        # return torch.tensor(image), torch.tensor(label)\n",
    "        return image.clone().detach(), label.clone().detach()\n",
    "\n",
    "\n",
    "def local_trainer(dataloader, model, local_epoch):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    model.train()\n",
    "    epoch_loss = []\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    for iter in range(local_epoch):\n",
    "        batch_loss = []\n",
    "        for batch_idx, (image, labels) in enumerate(dataloader):\n",
    "            images, labels = image.to(device), labels.to(device)\n",
    "            model.zero_grad()\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                print('| Local Epoch : {} | [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    iter, batch_idx * len(images),\n",
    "                    len(dataloader.dataset),\n",
    "                    100. * batch_idx / len(dataloader), loss.item()))\n",
    "            batch_loss.append(loss.item())\n",
    "        epoch_loss.append(sum(batch_loss) / len(batch_loss))\n",
    "    return model.state_dict(), sum(epoch_loss) / len(epoch_loss)\n",
    "\n",
    "\n",
    "def inference(model, testloader, device):\n",
    "    \"\"\" Returns the inference accuracy and loss.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    loss, total, correct = 0.0, 0.0, 0.0\n",
    "    for batch_idx, (images, labels) in enumerate(testloader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Inference\n",
    "        outputs = model(images)\n",
    "        batch_loss = criterion(outputs, labels)\n",
    "        loss += batch_loss.item()\n",
    "\n",
    "        # Prediction\n",
    "        _, pred_labels = torch.max(outputs, 1)\n",
    "        pred_labels = pred_labels.view(-1)\n",
    "        correct += torch.sum(torch.eq(pred_labels, labels)).item()\n",
    "        total += len(labels)\n",
    "    loss /= batch_idx\n",
    "    accuracy = correct / total\n",
    "    return accuracy, loss\n",
    "\n",
    "\n",
    "def average_weights(w):\n",
    "    \"\"\"\n",
    "    Returns the average of the weights.\n",
    "    \"\"\"\n",
    "    w_avg = copy.deepcopy(w[0])\n",
    "    for key in w_avg.keys():\n",
    "        for i in range(1, len(w)):\n",
    "            w_avg[key] += w[i][key]\n",
    "        w_avg[key] = torch.div(w_avg[key].float(), len(w))\n",
    "    return w_avg\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # prepare the train dataset\n",
    "    train_dataset, test_dataset = get_bank_dataset()\n",
    "    # split the dataset with dirichlet distribution\n",
    "    user_num = 5\n",
    "    global_rounds = 5\n",
    "    local_epochs = 10\n",
    "    alpha_acc = []\n",
    "    # for alpha in np.arange(0.1,2,0.5):\n",
    "    # alpha = 0.1\n",
    "    # train_index, test_index = dirichlet_partition(\n",
    "    #     train_dataset, test_dataset, alpha=alpha, user_num=user_num)\n",
    "    train_index = bank_iid(train_dataset, user_num)\n",
    "    test_index = bank_iid(test_dataset, user_num)\n",
    "    # train_index = mnist_noniid(train_dataset,user_num)\n",
    "    # test_index = mnist_noniid(test_dataset,user_num)\n",
    "    train_data_list = []\n",
    "    for user_index in range(user_num):\n",
    "        train_data_list.append(DatasetSplit(\n",
    "            train_dataset, train_index[user_index]))\n",
    "    # prepare the test data\n",
    "    batch_size = 32\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "    # define the model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # global_model = LeNet(10).to(device)\n",
    "    global_model = NeuralNet().to(device)\n",
    "    global_model.train()\n",
    "    # start federated learning\n",
    "    global_loss, global_acc = [],[]\n",
    "    for round_idx in range(global_rounds):\n",
    "        local_weights, local_losses = [], []\n",
    "        # global_acc = []\n",
    "        global_model.train()\n",
    "        for user_index in range(user_num):\n",
    "            train_dataloader = DataLoader(\n",
    "                train_data_list[user_index], batch_size=batch_size, shuffle=True)\n",
    "            # local train\n",
    "            model_weights, loss, _ = train(train_dataloader, copy.deepcopy(\n",
    "                global_model), local_epochs)\n",
    "            local_weights.append(copy.deepcopy(model_weights))\n",
    "            local_losses.append(copy.deepcopy(loss))\n",
    "\n",
    "        global_weight = average_weights(local_weights)\n",
    "        # update the global weights.\n",
    "        global_model.load_state_dict(global_weight)\n",
    "\n",
    "        test_acc, test_loss = inference(\n",
    "            global_model, test_loader, device=device)\n",
    "        print('Global Round :{}, the global accuracy is {:.3}%, and the global loss is {:.3}.'.format(\n",
    "            round_idx, 100 * test_acc, test_loss))\n",
    "        global_acc.append(test_acc)\n",
    "        global_loss.append(test_loss)\n",
    "    \n",
    "        # plot the image\n",
    "        # import matplotlib.pyplot as plt\n",
    "        # plt.figure()\n",
    "        # plt.title('Loss vs Rounds')\n",
    "        # plt.plot(range(len(global_loss)), global_loss, color='r')\n",
    "        # plt.ylabel('Loss')\n",
    "        # plt.xlabel('Rounds')\n",
    "\n",
    "        # plt.figure()\n",
    "        # plt.title('Acc vs Rounds')\n",
    "        # plt.plot(range(len(global_acc)), global_acc, color='r')\n",
    "        # plt.ylabel('Acc')\n",
    "        # plt.xlabel('Rounds')\n",
    "\n",
    "        # g_acc, g_loss = inference(\n",
    "        #         global_model, test_loader, device=device)\n",
    "        # alpha_acc.append(g_acc)\n",
    "    # import matplotlib.pyplot as plt\n",
    "    # plt.figure()\n",
    "    # plt.title('Acc vs Alpha')\n",
    "    # plt.plot(np.arange(0.1,2,0.5), alpha_acc, color='r')\n",
    "    # plt.ylabel('Acc')\n",
    "    # plt.xlabel('Alpha')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
