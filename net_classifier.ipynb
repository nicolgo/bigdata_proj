{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 587,
   "id": "53c37ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "id": "28241c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_dataset = pd.read_csv('BankChurners.csv')  \n",
    "#application = pd.read_csv('New_BankChurners.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "id": "1007ed6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15762418 'Spain' 3 ... 128643.35 1 8]\n",
      " [15749905 'Spain' 6 ... 50213.81 1 7]\n",
      " [15600911 'France' 2 ... 3061.0 0 7]\n",
      " ...\n",
      " [15636388 'Germany' 7 ... 114603.96 0 7]\n",
      " [15688951 'Germany' 8 ... 148412.24 1 9]\n",
      " [15581229 'Germany' 1 ... 122763.95 0 4]]\n"
     ]
    }
   ],
   "source": [
    "print(train_valid_dataset1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "id": "7607b63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_dataset1 = train_valid_dataset.to_numpy()\n",
    "weight_tensor = torch.tensor([len(train_valid_dataset1)/(len(train_valid_dataset1)-train_valid_dataset1[:,-1].sum()), \n",
    "                              len(train_valid_dataset1)/train_valid_dataset1[:,-1].sum()]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "id": "b9c885e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1861,  0.1569])\n"
     ]
    }
   ],
   "source": [
    "print(weight_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "id": "a15ef07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Tenure   Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
      "0        0.3  0.484985       0.000000        1.0             0.0   \n",
      "1        0.6  0.000000       0.000000        1.0             0.0   \n",
      "2        0.2  0.728934       0.000000        1.0             0.0   \n",
      "3        0.2  0.407651       0.333333        1.0             0.0   \n",
      "4        0.7  0.435819       0.333333        1.0             0.0   \n",
      "...      ...       ...            ...        ...             ...   \n",
      "8995     0.3  0.000000       0.000000        1.0             1.0   \n",
      "8996     0.6  0.000000       0.333333        1.0             1.0   \n",
      "8997     0.7  0.393687       0.000000        1.0             0.0   \n",
      "8998     0.8  0.476905       0.333333        0.0             1.0   \n",
      "8999     0.1  0.690881       0.000000        0.0             1.0   \n",
      "\n",
      "      EstimatedSalary  Exited  CreditLevel  \n",
      "0            0.643290     1.0            7  \n",
      "1            0.251062     1.0            6  \n",
      "2            0.015250     0.0            6  \n",
      "3            0.449146     0.0            1  \n",
      "4            0.513377     0.0            6  \n",
      "...               ...     ...          ...  \n",
      "8995         0.240535     0.0            5  \n",
      "8996         0.214993     0.0            5  \n",
      "8997         0.573079     0.0            6  \n",
      "8998         0.742155     1.0            8  \n",
      "8999         0.613887     0.0            3  \n",
      "\n",
      "[9000 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "train_valid_dataset = train_valid_dataset.drop(columns='CustomerId')\n",
    "train_valid_dataset = train_valid_dataset.drop(columns='Geography')\n",
    "res_col = train_valid_dataset['CreditLevel']\n",
    "train_valid_dataset = train_valid_dataset.drop(columns='CreditLevel')\n",
    "\n",
    "#归一化\n",
    "train_valid_dataset = (train_valid_dataset - train_valid_dataset.min()) / (train_valid_dataset.max() - train_valid_dataset.min())\n",
    "train_valid_dataset['CreditLevel'] = res_col - 1\n",
    "\n",
    "\n",
    "print(train_valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "id": "64a7f040",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ratio = 0.2 \n",
    "\n",
    "#weight_tensor = torch.tensor([len(train_valid_dataset)/(len(train_valid_dataset)-train_valid_dataset[:,-1].sum()), len(train_valid_dataset)/train_valid_dataset[:,-1].sum()]).float() \n",
    "\n",
    "nb_train = int((1.0 - valid_ratio) * len(train_valid_dataset))\n",
    "nb_valid =  int(valid_ratio * len(train_valid_dataset))\n",
    "train_dataset, valid_dataset = torch.utils.data.dataset.random_split(train_valid_dataset.to_numpy(), [nb_train, nb_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "id": "41e9679d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetTransformer(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, base_dataset, transform=transforms.Lambda(lambda x: x)):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        inpt, target = torch.from_numpy(self.base_dataset[index][:-1]), self.base_dataset[index][-1]\n",
    "        return self.transform(inpt).float(), int(target)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset)\n",
    "\n",
    "\n",
    "train_dataset = DatasetTransformer(train_dataset)\n",
    "valid_dataset = DatasetTransformer(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "id": "ae6ec87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train set contains 7200 samples, in 72 batches\n",
      "The validation set contains 1800 samples, in 18 batches\n"
     ]
    }
   ],
   "source": [
    "#Dataloader\n",
    "\n",
    "batch_size  = 100   # Using minibatches of X samples\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "\n",
    "print(\"The train set contains {} samples, in {} batches\".format(len(train_loader.dataset), len(train_loader)))\n",
    "print(\"The validation set contains {} samples, in {} batches\".format(len(valid_loader.dataset), len(valid_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "id": "f2a91c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define device\n",
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "id": "7cb8b3b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FullyConnected(\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=7, out_features=14, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=14, out_features=28, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Linear(in_features=28, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 622,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def linear_relu(dim_in, dim_out):\n",
    "    return [nn.Linear(dim_in, dim_out),\n",
    "            nn.ReLU(inplace=True)]\n",
    "\n",
    "class FullyConnected(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(FullyConnected, self).__init__()\n",
    "        self.classifier =  nn.Sequential(\n",
    "            #nn.Dropout(0.2),\n",
    "            *linear_relu(input_size, 14),\n",
    "            #nn.Dropout(0.5), #Generally 0.2 for the input layer and 0.5 for the hidden layer\n",
    "            *linear_relu(14, 28),\n",
    "            #nn.Dropout(0.5),\n",
    "            nn.Linear(28, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        y = self.classifier(x)\n",
    "        return y\n",
    "\n",
    "\n",
    "model = FullyConnected(7, 10)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "id": "3d0b5296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, f_loss, optimizer, device):\n",
    "    \"\"\"\n",
    "    Train a model for one epoch, iterating over the loader\n",
    "    using the f_loss to compute the loss and the optimizer\n",
    "    to update the parameters of the model.\n",
    "\n",
    "    Arguments :\n",
    "\n",
    "        model     -- A torch.nn.Module object\n",
    "        loader    -- A torch.utils.data.DataLoader\n",
    "        f_loss    -- The loss function, i.e. a loss Module\n",
    "        optimizer -- A torch.optim.Optimzer object\n",
    "        device    -- a torch.device class specifying the device\n",
    "                     used for computation\n",
    "\n",
    "    Returns :\n",
    "    \"\"\"\n",
    "\n",
    "    # We enter train mode. This is useless for the linear model\n",
    "    # but is important for layers such as dropout, batchnorm, ...\n",
    "    model.train()\n",
    "\n",
    "    for i, (inputs, targets) in enumerate(loader):\n",
    "        \n",
    "        \n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        # Compute the forward pass through the network up to the loss\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        #print(targets)\n",
    "        #print(outputs)\n",
    "        \n",
    "        loss = f_loss(outputs, targets)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "def test(model, loader, f_loss, device):\n",
    "    \"\"\"\n",
    "    Test a model by iterating over the loader\n",
    "\n",
    "    Arguments :\n",
    "\n",
    "        model     -- A torch.nn.Module object\n",
    "        loader    -- A torch.utils.data.DataLoader\n",
    "        f_loss    -- The loss function, i.e. a loss Module\n",
    "        device    -- The device to use for computation \n",
    "\n",
    "    Returns :\n",
    "\n",
    "        A tuple with the mean loss, mean accuracy and mean unbiaised accuracy\n",
    "\n",
    "    \"\"\"\n",
    "    # We disable gradient computation which speeds up the computation\n",
    "    # and reduces the memory usage\n",
    "    with torch.no_grad():\n",
    "        # We enter evaluation mode. This is useless for the linear model\n",
    "        # but is important with layers such as dropout, batchnorm, ..\n",
    "        model.eval()\n",
    "        N = 0\n",
    "        tot_loss, correct, unbiaised_acc = 0.0, 0.0, 0.0\n",
    "        for i, (inputs, targets) in enumerate(loader):\n",
    "\n",
    "            # We got a minibatch from the loader within inputs and targets\n",
    "\n",
    "            # We need to copy the data on the GPU if we use one\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # Compute the forward pass, i.e. the scores for each input\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # We accumulate the exact number of processed samples\n",
    "            N += inputs.shape[0]\n",
    "\n",
    "            # We accumulate the loss considering\n",
    "            # The multipliation by inputs.shape[0] is due to the fact\n",
    "            # that our loss criterion is averaging over its samples\n",
    "            tot_loss += inputs.shape[0] * f_loss(outputs, targets).item()\n",
    "\n",
    "            # For the accuracy, we compute the labels for each input\n",
    "            # Be carefull, the model is outputing scores and not the probabilities\n",
    "            # But given the softmax is not altering the rank of its input scores\n",
    "            # we can compute the label by argmaxing directly the scores\n",
    "            predicted_targets = outputs.argmax(dim=1)\n",
    "            \n",
    "            correct += (predicted_targets == targets).sum().item()\n",
    "            \n",
    "            #Compute the unbiaised accuracy\n",
    "            for value in predicted_targets.unique() :\n",
    "                mask = (predicted_targets == targets) & (predicted_targets == value.item())\n",
    "                \n",
    "        return tot_loss/N, correct/N, unbiaised_acc/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "id": "e89a3ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelCheckpoint:\n",
    "\n",
    "    def __init__(self, filepath, model):\n",
    "        self.min_loss = None\n",
    "        self.filepath = filepath\n",
    "        self.model = model\n",
    "\n",
    "    def update(self, loss):\n",
    "        if (self.min_loss is None) or (loss < self.min_loss):\n",
    "            print(\"Saving a better model\")\n",
    "            torch.save(self.model.state_dict(), self.filepath)\n",
    "            self.min_loss = loss\n",
    "            \n",
    "            \n",
    "model_path = \"best_model.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "id": "ca9053fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "id": "4758c565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "weight tensor should be defined either for all 10 classes or no classes but got weight tensor of shape: [2]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-626-3e04877d75fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nEpoch {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_unb_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" Train : Loss : {:.4f}, Acc : {:.4f}, Unb.Acc. : {:.4f}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_unb_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-623-cca37f8905ad>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, loader, f_loss, optimizer, device)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;31m#print(outputs)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;31m# Backward and optimize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1150\u001b[1;33m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[0;32m   1151\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1152\u001b[0m                                label_smoothing=self.label_smoothing)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   2844\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2845\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2846\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2847\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2848\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: weight tensor should be defined either for all 10 classes or no classes but got weight tensor of shape: [2]"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "f_loss = torch.nn.CrossEntropyLoss()\n",
    "f_loss = torch.nn.CrossEntropyLoss(weight=weight_tensor.to(device))\n",
    "model_checkpoint = ModelCheckpoint(model_path, model)\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(\"\\nEpoch {}\".format(t))\n",
    "    train(model, train_loader, f_loss, optimizer, device)\n",
    "    train_loss, train_acc, train_unb_acc = test(model, train_loader, f_loss, device)\n",
    "    print(\" Train : Loss : {:.4f}, Acc : {:.4f}, Unb.Acc. : {:.4f}\".format(train_loss, train_acc, train_unb_acc))\n",
    "\n",
    "    val_loss, val_acc, val_unb_acc = test(model, valid_loader, f_loss, device)\n",
    "    print(\" Validation : Loss : {:.4f}, Acc : {:.4f}, Unb.Acc. : {:.4f}\".format(val_loss, val_acc, val_unb_acc))\n",
    "\n",
    "    model_checkpoint.update(val_loss)\n",
    "\n",
    "\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# Switch to eval mode \n",
    "model.eval()\n",
    "\n",
    "test_loss, test_acc, test_unb_acc = test(model, valid_loader, f_loss, device)\n",
    "print(\"\\n\\n Test : Loss : {:.4f}, Acc. : {:.4f}, Unb.Acc. : {:.4f}\".format(test_loss, test_acc, test_unb_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd78bc34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34d7a56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63681323",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
